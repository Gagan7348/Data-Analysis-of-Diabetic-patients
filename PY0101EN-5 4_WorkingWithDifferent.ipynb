{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import piplite\n",
        "await piplite.install(['seaborn', 'lxml', 'openpyxl'])\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pyodide.http import pyfetch\n",
        "\n",
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/addresses.csv\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"addresses.csv\")\n",
        "\n",
        "df = pd.read_csv(\"addresses.csv\", header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.columns =['First Name', 'Last Name', 'Location ', 'City','State','Area Code']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df[\"First Name\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Selecting multiple columns\n",
        "\n",
        "To select multiple columns, you can pass a list of column names to the indexing operator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = df[['First Name', 'Last Name', 'Location ', 'City','State','Area Code']]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Selecting rows using .iloc and .loc\n",
        "\n",
        "Now, let's see how to use .loc for selecting rows from our DataFrame.\n",
        "\n",
        "**loc() : loc() is label based data selecting method which means that we have to pass the name of the row or column which we want to select.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# To select the first row\n",
        "df.loc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# To select the 0th,1st and 2nd row of \"First Name\" column only\n",
        "df.loc[[0,1,2], \"First Name\" ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's see how to use .iloc for selecting rows from our DataFrame.\n",
        "\n",
        "**iloc() : iloc() is a indexed based selecting method which means that we have to pass integer index in the method to select specific row/column.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# To select the 0th,1st and 2nd row of \"First Name\" column only\n",
        "df.iloc[[0,1,2], 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#import library\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#creating a dataframe\n",
        "df=pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s say we want to add 10 to each element in a dataframe:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#applying the transform function\n",
        "df = df.transform(func = lambda x : x + 10)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will use DataFrame.transform() function to find the square root to each element of the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "result = df.transform(func = ['sqrt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more information about the **transform()** function  please read the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The text in JSON is done through quoted string which contains the values in key-value mappings within { }. It is similar to the dictionary in Python.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Python supports JSON through a built-in package called **json**. To use this feature, we import the json package in Python script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json\n",
        "person = {\n",
        "    'first_name' : 'Mark',\n",
        "    'last_name' : 'abc',\n",
        "    'age' : 27,\n",
        "    'address': {\n",
        "        \"streetAddress\": \"21 2nd Street\",\n",
        "        \"city\": \"New York\",\n",
        "        \"state\": \"NY\",\n",
        "        \"postalCode\": \"10021-3100\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### serialization using dump() function\n",
        "\n",
        "**json.dump()** method can be used for writing to JSON file.\n",
        "\n",
        "Syntax: json.dump(dict, file_pointer)\n",
        "\n",
        "Parameters:\n",
        "\n",
        "1.  **dictionary** – name of the dictionary which should be converted to JSON object.\n",
        "2.  **file pointer** – pointer of the file opened in write or append mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "with open('person.json', 'w') as f:  # writing JSON object\n",
        "    json.dump(person, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### serialization using dumps() function\n",
        "\n",
        "**json.dumps()** that helps in converting a dictionary to a JSON object.\n",
        "\n",
        "It takes two parameters:\n",
        "\n",
        "1.  **dictionary** – name of the dictionary which should be converted to JSON object.\n",
        "2.  **indent** – defines the number of units for indentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Serializing json  \n",
        "json_object = json.dumps(person, indent = 4) \n",
        "  \n",
        "# Writing to sample.json \n",
        "with open(\"sample.json\", \"w\") as outfile: \n",
        "    outfile.write(json_object) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(json_object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our Python objects are now serialized to the file. For deserialize it back to the Python object, we use the load() function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json \n",
        "  \n",
        "# Opening JSON file \n",
        "with open('sample.json', 'r') as openfile: \n",
        "  \n",
        "    # Reading from json file \n",
        "    json_object = json.load(openfile) \n",
        "  \n",
        "print(json_object) \n",
        "print(type(json_object)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Not needed unless you're running locally\n",
        "# import urllib.request\n",
        "# urllib.request.urlretrieve(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/file_example_XLSX_10.xlsx\", \"sample.xlsx\")\n",
        "\n",
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/file_example_XLSX_10.xlsx\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"file_example_XLSX_10.xlsx\")\n",
        "\n",
        "df = pd.read_excel(\"file_example_XLSX_10.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# create the file structure\n",
        "employee = ET.Element('employee')\n",
        "details = ET.SubElement(employee, 'details')\n",
        "first = ET.SubElement(details, 'firstname')\n",
        "second = ET.SubElement(details, 'lastname')\n",
        "third = ET.SubElement(details, 'age')\n",
        "first.text = 'Shiv'\n",
        "second.text = 'Mishra'\n",
        "third.text = '23'\n",
        "\n",
        "# create a new XML file with the results\n",
        "mydata1 = ET.ElementTree(employee)\n",
        "# myfile = open(\"items2.xml\", \"wb\")\n",
        "# myfile.write(mydata)\n",
        "with open(\"new_sample.xml\", \"wb\") as files:\n",
        "    mydata1.write(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Not needed unless running locally\n",
        "# !wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/Sample-employee-XML-file.xml\n",
        "\n",
        "import xml.etree.ElementTree as etree\n",
        "\n",
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/Sample-employee-XML-file.xml\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"Sample-employee-XML-file.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Parse the XML file\n",
        "tree = etree.parse(\"Sample-employee-XML-file.xml\")\n",
        "\n",
        "# Get the root of the XML tree\n",
        "root = tree.getroot()\n",
        "\n",
        "# Define the columns for the DataFrame\n",
        "columns = [\"firstname\", \"lastname\", \"title\", \"division\", \"building\", \"room\"]\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "datatframe = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Iterate through each node in the XML root\n",
        "for node in root:\n",
        "    # Extract text from each element\n",
        "    firstname = node.find(\"firstname\").text\n",
        "    lastname = node.find(\"lastname\").text\n",
        "    title = node.find(\"title\").text\n",
        "    division = node.find(\"division\").text\n",
        "    building = node.find(\"building\").text\n",
        "    room = node.find(\"room\").text\n",
        "    \n",
        "    # Create a DataFrame for the current row\n",
        "    row_df = pd.DataFrame([[firstname, lastname, title, division, building, room]], columns=columns)\n",
        "    \n",
        "    # Concatenate with the existing DataFrame\n",
        "    datatframe = pd.concat([datatframe, row_df], ignore_index=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "datatframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Herein xpath we mention the set of xml nodes to be considered for migrating  to the dataframe which in this case is details node under employees.\n",
        "df=pd.read_xml(\"Sample-employee-XML-file.xml\", xpath=\"/employees/details\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "datatframe.to_csv(\"employee.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Read/Save Other Data Formats</h2>\n",
        "\n",
        "| Data Formate |        Read       |            Save |\n",
        "| ------------ | :---------------: | --------------: |\n",
        "| csv          |  `pd.read_csv()`  |   `df.to_csv()` |\n",
        "| json         |  `pd.read_json()` |  `df.to_json()` |\n",
        "| excel        | `pd.read_excel()` | `df.to_excel()` |\n",
        "| hdf          |  `pd.read_hdf()`  |   `df.to_hdf()` |\n",
        "| sql          |  `pd.read_sql()`  |   `df.to_sql()` |\n",
        "| ...          |        ...        |             ... |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's move ahead and perform some **Data Analysis**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# importing PIL \n",
        "from PIL import Image \n",
        "\n",
        "# Uncomment if running locally\n",
        "# import urllib.request\n",
        "# urllib.request.urlretrieve(\"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/dog-puppy-on-garden-royalty-free-image-1586966191.jpg\", \"dog.jpg\")\n",
        "\n",
        "filename = \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/dog-puppy-on-garden-royalty-free-image-1586966191.jpg\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"./dog.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Read image \n",
        "img = Image.open('./dog.jpg','r') \n",
        "  \n",
        "# Output Images \n",
        "img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Import pandas library\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/diabetes.csv\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"diabetes.csv\")\n",
        "df = pd.read_csv(\"diabetes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# show the first 5 rows using dataframe.head() method\n",
        "print(\"The first 5 rows of the dataframe\") \n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To view the dimensions of the dataframe, we use the **`.shape`** parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Identify and handle missing values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use Python's built-in functions to identify these missing values. There are two methods to detect missing data:\n",
        "\n",
        "**.isnull()**\n",
        "\n",
        "**.notnull()**\n",
        "\n",
        "The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "missing_data = df.isnull()\n",
        "missing_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for column in missing_data.columns.values.tolist():\n",
        "    print(column)\n",
        "    print (missing_data[column].value_counts())\n",
        "    print(\"\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see above, there is no missing values in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see above, All columns have the correct data type.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "labels= 'Not Diabetic','Diabetic'\n",
        "plt.pie(df['Outcome'].value_counts(),labels=labels,autopct='%0.02f%%')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see above, 65.10% females are not Diabetic and 34.90% are Diabetic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!--## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By    | Change Description |\n",
        "| ----------------- | ------- | ------------- | ------------------ |\n",
        "| 2025-04-17 | 1.2 | Anita verma | Updated instructions |\n",
        "| 2023-11-02 | 1.1 | Abhishek Gagneja | Updated instructions |\n",
        "| 2023-06-11        | 1.0     | Akansha yadav   | Spell check\n",
        "| 2022-01-25        | 0.1     | Lakshmi Holla | added read_xml     |\n",
        "-->\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "prev_pub_hash": "a04e80168ae105af276ac73f3ecaa3d13151e085121e7c55d350cfbce8c34bf7"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
